{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student name : Wala'a Khalili\n",
    "# Student number: 403236"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  I have a twitter acount and inside the acount I initiate an app where i have assigned the dataset to be inside  \"https://api.twitter.com/1.1/search/tweets.json\" , all the retrieved data pulled from that url  that make using Tweepy library easy for processing twitter streaming, it's well documented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare all the needed libraries \n",
    "import json\n",
    "from tweepy import Stream\n",
    "from itertools import islice\n",
    "import time\n",
    "import numpy as np \n",
    "import tweepy  \n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials from https://apps.twitter.com/. make the authuntication using my consumer_key and token keys \n",
    "consumer_key = \"***************************\"\n",
    "consumer_secret = \"************************\"\n",
    "access_key = \"*************************\"\n",
    "access_secret = \"***************************************\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "\n",
    "api =  tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 780\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Twitter is designed in such a way which only aimed to post the tweets based on an event /user/keyword.\n",
    "the research should be confined to any particular domain.because of that I specified the search term to \"great\"\n",
    "even when I added the code tweepy.stream which is a continous stream I should also specify the search word in (track)\n",
    " \"\"\"\n",
    "#percentage of tweets mentioning “Yes” during 2 minutes, from all tweets, without\n",
    "#restrictions.\n",
    "\n",
    "\n",
    "search_words = \"great\"\n",
    "date_since = \"2020-05-22\"\n",
    "max_tweets = 2000 # for the last 2000 tweets \n",
    "start_time= datetime.now()\n",
    "yes_count=0\n",
    "\n",
    "# percentage of tweets mentioning “Yes” during 2 minutes, from all tweets, without restrictions.\n",
    "text=[]\n",
    "for tweet in tweepy.Cursor(api.search,\n",
    "                           q=search_words,\n",
    "                           result_type=\" recent\",\n",
    "                           include_entities=True,\n",
    "                           lang=\" \",\n",
    "                           untill=date_since).items():\n",
    "#     print (tweet.created_at, tweet.text)\n",
    "#     print(tweet.text)\n",
    "    text.append(tweet.text)\n",
    "    end_time = datetime.now()\n",
    "    if np.ceil((end_time - start_time).seconds/60)>=2:  # two minutes \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.22271714922048996 %\n"
     ]
    }
   ],
   "source": [
    "# we need to count the pearcentage of appearning \"yes\" in the text\n",
    "\n",
    "yes_count=0\n",
    "for item in text:\n",
    "    index= item.split()\n",
    "    if index.count(\"yes\") or index.count(\"Yes\"):\n",
    "        yes_count+=1\n",
    "#number of counted yes \n",
    "print(yes_count)\n",
    "\n",
    "# percentage of tweets mentioned yes  \n",
    "\n",
    "yes_percentage= 100*(yes_count/len(text))\n",
    "print(yes_percentage,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sliding window on the same text that have been generated form the query above\n",
    "\n",
    "def slide_window(seq, n=300):\n",
    "                  \n",
    "    it = iter(seq)\n",
    "    result = tuple(islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result    \n",
    "    for elem in it:\n",
    "        if \"yes\" in elem:\n",
    "            result = result[1:] + (elem,)\n",
    "            yield result\n",
    "        \n",
    "windowed= slide_window(text,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22271714922048996 %\n"
     ]
    }
   ],
   "source": [
    "print(100*(len(list(windowed))/len(text)),\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.209423154733535 %\n"
     ]
    }
   ],
   "source": [
    "# (batching) percentage of tweets mentioning “No” during 2 minutes. Assume that the\n",
    "# counting function has a very slow rate, use buffers (length 100) for the elements and\n",
    "# compute the query answer using each tweet once.\n",
    "\n",
    "\n",
    "\"\"\" tweepy has a magnificent attributes that is maximum number of statuses which act as a buffer.\n",
    "for continuous query we let the code sleep for (time.sleep(920)) for 15 minutes and ask again.\n",
    "for data we can get data from where we have stopped by using since_id and max_id these attributes helps\n",
    "getting data from where we have stopped\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "search_words = \"great\"\n",
    "date_since = \"2020-05-25\"\n",
    "max_tweets =100 # buffering for 100 \n",
    "min_id= 1125490788736032770            # assumed number \n",
    "maximum_id=2125490788736032770       # assumed number \n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# percentage of tweets mentioning “Yes” during 2 minutes, from all tweets, without restrictions.\n",
    "# text=[]\n",
    "no_count=0\n",
    "no_percentage=0\n",
    "for tweet in tweepy.Cursor(api.search,\n",
    "                           q=search_words,\n",
    "                           count=5,\n",
    "                           result_type=\"recent\",\n",
    "                           include_entities=True,\n",
    "                           lang=\"en\",\n",
    "                           since_id=min_id ,\n",
    "                           max_id=maximum_id,\n",
    "                           untill=date_since).items(max_tweets):\n",
    "    text= tweet.text\n",
    "    \n",
    "    index = text.split()\n",
    "    if index.count(\"no\") or index.count(\"No\"):\n",
    "         no_count+=1\n",
    "\n",
    "    no_percentage+= 100*(no_count/len(text))\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    if np.ceil((end_time - start_time).seconds/60)==2:  # retrieve data for two minutes \n",
    "        break \n",
    "#         time.sleep(15*60)\n",
    "#         start_time = datetime.now()\n",
    "#         end_time = datetime.now()\n",
    "          \n",
    "    maximum_id=min_id\n",
    "    min_id+=20000      # assumed number \n",
    "    \n",
    "print(no_percentage,\"%\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" (sampling) percentage of tweets mentioning “Hi” during 2 minutes. Assume that the update function is slow.\n",
    "Update the list with a sample of the elements. (e.g., one in one hundred).\n",
    " \"\"\"\n",
    "\n",
    "# Define the search term and the date_since date as variables\n",
    "search_words = \"great\"\n",
    "# date_since = \"2020-05-20\"\n",
    "max_tweets = 2000 # for the last 2000 tweets \n",
    "\n",
    "# percentage of tweets mentioning “Yes” during 2 minutes, from all tweets, without restrictions.\n",
    "text=[]\n",
    "n=10\n",
    "HI_count=0\n",
    "count=0\n",
    "start_time=datetime.now()\n",
    "for tweet in tweepy.Cursor(api.search,\n",
    "                           q=search_words,\n",
    "                           result_type=\" recent\",\n",
    "                           include_entities=True,\n",
    "                           lang=\" \",\n",
    "                           until=\" \").items():\n",
    "#     print (tweet.created_at, tweet.text)\n",
    "#     print(tweet.text)\n",
    "\n",
    "# take samples one for every one hundred tweet\n",
    "    if count%n == 0:\n",
    "        text.append(tweet.text)\n",
    "    count+=1\n",
    "    end_time = datetime.now()\n",
    "    if np.ceil((end_time - start_time).seconds/60)>=2:  # two minutes \n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8968609865470852 %\n"
     ]
    }
   ],
   "source": [
    "# counting Hi mentioned in the text \n",
    "Hi_count=0\n",
    "for item in text:\n",
    "    index = item.split()\n",
    "    if index.count(\"hi\") or index.count(\"Hi\"):\n",
    "        Hi_count+=1\n",
    "\n",
    "Hi_percentage= 100*(Hi_count/len(text))\n",
    "print(Hi_percentage,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "A treasure map to Twitter Data via Tweepy https://medium.com/analytics-vidhya/a-treasure-map-to-twitter-data-via-tweepy-b7e3d624d88d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
